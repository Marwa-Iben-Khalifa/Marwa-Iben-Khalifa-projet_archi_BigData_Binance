{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d3b323f-ec2c-4b0c-bcc8-d5328b23d23e",
   "metadata": {},
   "source": [
    "# Projet Archi BigData\n",
    "\n",
    "## équipe\n",
    "- Marwa Iben Khalifa\n",
    "- Maxime Cordeiro\n",
    "- Louis Poulin\n",
    "- Ivan Braga Fernandes\n",
    "- Hicham Hecharif Chefchaouni\n",
    "\n",
    "## client\n",
    "Binance\n",
    "\n",
    "# contraintes\n",
    "- serveur mongodb\n",
    "- cluster Hadoop HDFS\n",
    "- processing des données avec Apache Spark\n",
    "\n",
    "# livrables demandés\n",
    "- un support de présentation (ex: powerpoint)\n",
    "- méthodologie\n",
    "- présentation du sujet + source de donnée\n",
    "- présentation de l’architecture (ex: draw io)\n",
    "- justification des choix de technos\n",
    "- difficultés rencontrées, axe d’amélioration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0360aa7f-37e1-473a-b579-df441dd8780d",
   "metadata": {},
   "source": [
    "## Chargement de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "570a76fd-986f-4517-9bc9-cabb1a84ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "appName= \"hive_pyspark\"\n",
    "master= \"local\"\n",
    "spark = SparkSession.builder \\\n",
    "        .master(master).appName(appName).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "116c9826",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/BTCBUSD-4h-2022-11-22.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe9e6be-752d-4ceb-bf7c-80739a06564c",
   "metadata": {},
   "source": [
    "## ajout du header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85847877-82eb-40b5-8bcc-61ae1648c6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+--------+--------+--------+---------+-------------+------------------+----------------+---------------------------+----------------------------+------+\n",
      "|    timestamp|    open|    high|     low|   close|   volume|   close_time|quote_asset_volume|number_of_trades|taker_buy_base_asset_volume|taker_buy_quote_asset_volume|ignore|\n",
      "+-------------+--------+--------+--------+--------+---------+-------------+------------------+----------------+---------------------------+----------------------------+------+\n",
      "|1669075200000|15761.55|15955.62|15726.98|15828.25|20762.799|1669089599999|      3.28859616E8|          491909|                  10147.984|                  1.607348E8|     0|\n",
      "|1669089600000|15828.28|15862.66| 15598.6|15742.88|20869.232|1669103999999|      3.28515104E8|          452242|                  10223.968|                 1.6093504E8|     0|\n",
      "|1669104000000|15742.88|15805.61|15638.63| 15730.0|20420.682|1669118399999|       3.2082464E8|          451824|                   9941.303|                1.56196976E8|     0|\n",
      "|1669118400000|15730.06| 16294.0|15666.66|16225.21|  41892.7|1669132799999|       6.7174394E8|          789632|                  21104.174|                3.38449504E8|     0|\n",
      "|1669132800000|16223.65| 16290.0|16029.44|16149.54|22559.574|1669147199999|      3.64239296E8|          480970|                  11167.281|                1.80308176E8|     0|\n",
      "+-------------+--------+--------+--------+--------+---------+-------------+------------------+----------------+---------------------------+----------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType() \\\n",
    "        .add(\"timestamp\",StringType(),True) \\\n",
    "        .add(\"open\",FloatType(),True) \\\n",
    "        .add(\"high\",FloatType(),True) \\\n",
    "        .add(\"low\",FloatType(),True) \\\n",
    "        .add(\"close\",FloatType(),True) \\\n",
    "        .add(\"volume\",FloatType(),True) \\\n",
    "        .add(\"close_time\",StringType(),True) \\\n",
    "        .add(\"quote_asset_volume\",FloatType(),True) \\\n",
    "        .add(\"number_of_trades\",IntegerType(),True) \\\n",
    "        .add(\"taker_buy_base_asset_volume\",FloatType(),True) \\\n",
    "        .add(\"taker_buy_quote_asset_volume\",FloatType(),True) \\\n",
    "        .add(\"ignore\",IntegerType(),True)\n",
    "      \n",
    "df_binance = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(path)\n",
    "df_binance.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "344f1cde-5009-45da-a502-a5c48f7867a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hive=spark.sql(\"show databases\")\n",
    "df_hive.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6664d8c6-f437-4ff0-9439-00b9c9261636",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binance.write.saveAsTable(\"binance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d0616e5c-68b4-455f-815a-739a59c09dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+--------+--------+--------+---------+-------------+------------------+----------------+---------------------------+----------------------------+------+\n",
      "|    timestamp|    open|    high|     low|   close|   volume|   close_time|quote_asset_volume|number_of_trades|taker_buy_base_asset_volume|taker_buy_quote_asset_volume|ignore|\n",
      "+-------------+--------+--------+--------+--------+---------+-------------+------------------+----------------+---------------------------+----------------------------+------+\n",
      "|1669075200000|15761.55|15955.62|15726.98|15828.25|20762.799|1669089599999|      3.28859616E8|          491909|                  10147.984|                  1.607348E8|     0|\n",
      "|1669089600000|15828.28|15862.66| 15598.6|15742.88|20869.232|1669103999999|      3.28515104E8|          452242|                  10223.968|                 1.6093504E8|     0|\n",
      "|1669104000000|15742.88|15805.61|15638.63| 15730.0|20420.682|1669118399999|       3.2082464E8|          451824|                   9941.303|                1.56196976E8|     0|\n",
      "|1669118400000|15730.06| 16294.0|15666.66|16225.21|  41892.7|1669132799999|       6.7174394E8|          789632|                  21104.174|                3.38449504E8|     0|\n",
      "|1669132800000|16223.65| 16290.0|16029.44|16149.54|22559.574|1669147199999|      3.64239296E8|          480970|                  11167.281|                1.80308176E8|     0|\n",
      "+-------------+--------+--------+--------+--------+---------+-------------+------------------+----------------+---------------------------+----------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.sql(\"select * from binance limit 5\")\n",
    "df1.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e0af220dfa4f1ff70f8ab7dc1728e5d5c0d34bac1849ca2a7546fe8ac61926e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
